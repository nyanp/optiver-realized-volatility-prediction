{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "optuna optiver.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "305c6e94",
        "outputId": "bba0070b-2730-43ca-c9d4-b54164671c40"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "#from google.colab import auth\n",
        "from google.colab import drive\n",
        "#auth.authenticate_user()\n",
        "drive.mount('/content/drive')\n"
      ],
      "id": "305c6e94",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APbpoKoqSQyR"
      },
      "source": [
        "!cp /content/drive/MyDrive/optiver-realized-volatility-prediction/input/optiver_df2.f ./\n",
        "!cp /content/drive/MyDrive/optiver-realized-volatility-prediction/input/time_id_order.csv ./"
      ],
      "id": "APbpoKoqSQyR",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgqIc3U_TNFq"
      },
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.backend import sigmoid\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "@contextmanager\n",
        "def timer(name: str):\n",
        "    s = time.time()\n",
        "    yield\n",
        "    elapsed = time.time() - s\n",
        "    print(f'[{name}] {elapsed: .3f}sec')  "
      ],
      "id": "rgqIc3U_TNFq",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvrxDzRTVX5",
        "outputId": "9a7b0ed3-a592-4c83-8475-709cc6dcf281"
      },
      "source": [
        "def prepare_df():\n",
        "    df = pd.read_feather('optiver_df2.f')\n",
        "    df_train = df[~df.target.isnull()].copy()\n",
        "    stock_ids = df_train['stock_id']\n",
        "\n",
        "    del df\n",
        "\n",
        "    with timer('make folds'):\n",
        "        timeid_order = pd.read_csv('time_id_order.csv')\n",
        "\n",
        "        timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
        "        df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
        "        df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
        "\n",
        "        folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n",
        "        time_id_orders = df_train['time_id_order']\n",
        "\n",
        "        folds = []\n",
        "        for i, border in enumerate(folds_border):\n",
        "            idx_train = np.where(time_id_orders < border)[0]\n",
        "            idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n",
        "            folds.append((idx_train, idx_valid))\n",
        "            \n",
        "            print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n",
        "            \n",
        "    del df_train['time_id_order']\n",
        "\n",
        "    return df_train, folds\n",
        "\n",
        "df_train, folds = prepare_df()"
      ],
      "id": "ezvrxDzRTVX5",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "folds0: train=257362, valid=42882\n",
            "folds1: train=300244, valid=42896\n",
            "folds2: train=343140, valid=42896\n",
            "folds3: train=386036, valid=42896\n",
            "[make folds]  2.033sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ_GYky-_J0L",
        "outputId": "014b7f41-c9bf-4c92-c867-ea19c7890623"
      },
      "source": [
        "!pip install pytorch-tabnet==2.0.1"
      ],
      "id": "UZ_GYky-_J0L",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-tabnet==2.0.1 in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==2.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==2.0.1) (1.19.5)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==2.0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==2.0.1) (4.62.2)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==2.0.1) (1.9.0+cu102)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet==2.0.1) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet==2.0.1) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a33d5397"
      },
      "source": [
        "\n",
        "import gc\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import List, Tuple, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.special import erf, erfinv\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n",
        "from sklearn.decomposition import PCA\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "\n",
        "null_check_cols = [\n",
        "    'book.log_return1.realized_volatility',\n",
        "    'book_150.log_return1.realized_volatility',\n",
        "    'book_300.log_return1.realized_volatility',\n",
        "    'book_450.log_return1.realized_volatility',\n",
        "    'trade.log_return.realized_volatility',\n",
        "    'trade_150.log_return.realized_volatility',\n",
        "    'trade_300.log_return.realized_volatility',\n",
        "    'trade_450.log_return.realized_volatility'\n",
        "]\n",
        "\n",
        "class GaussRankScaler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Transform features by scaling each feature to a normal distribution.\n",
        "    Parameters\n",
        "        ----------\n",
        "        epsilon : float, optional, default 1e-4\n",
        "            A small amount added to the lower bound or subtracted\n",
        "            from the upper bound. This value prevents infinite number\n",
        "            from occurring when applying the inverse error function.\n",
        "        copy : boolean, optional, default True\n",
        "            If False, try to avoid a copy and do inplace scaling instead.\n",
        "            This is not guaranteed to always work inplace; e.g. if the data is\n",
        "            not a NumPy array, a copy may still be returned.\n",
        "        n_jobs : int or None, optional, default None\n",
        "            Number of jobs to run in parallel.\n",
        "            ``None`` means 1 and ``-1`` means using all processors.\n",
        "        interp_kind : str or int, optional, default 'linear'\n",
        "           Specifies the kind of interpolation as a string\n",
        "            ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
        "            'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n",
        "            refer to a spline interpolation of zeroth, first, second or third\n",
        "            order; 'previous' and 'next' simply return the previous or next value\n",
        "            of the point) or as an integer specifying the order of the spline\n",
        "            interpolator to use.\n",
        "        interp_copy : bool, optional, default False\n",
        "            If True, the interpolation function makes internal copies of x and y.\n",
        "            If False, references to `x` and `y` are used.\n",
        "        Attributes\n",
        "        ----------\n",
        "        interp_func_ : list\n",
        "            The interpolation function for each feature in the training set.\n",
        "        \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n",
        "        self.epsilon = epsilon\n",
        "        self.copy = copy\n",
        "        self.interp_kind = interp_kind\n",
        "        self.interp_copy = interp_copy\n",
        "        self.fill_value = 'extrapolate'\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit interpolation function to link rank with original data for future scaling\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The data used to fit interpolation function for later scaling along the features axis.\n",
        "        y\n",
        "            Ignored\n",
        "        \"\"\"\n",
        "        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
        "\n",
        "        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit)(x) for x in X.T)\n",
        "        return self\n",
        "\n",
        "    def _fit(self, x):\n",
        "        x = self.drop_duplicates(x)\n",
        "        rank = np.argsort(np.argsort(x))\n",
        "        bound = 1.0 - self.epsilon\n",
        "        factor = np.max(rank) / 2.0 * bound\n",
        "        scaled_rank = np.clip(rank / factor - bound, -bound, bound)\n",
        "        return interp1d(\n",
        "            x, scaled_rank, kind=self.interp_kind, copy=self.interp_copy, fill_value=self.fill_value)\n",
        "\n",
        "    def transform(self, X, copy=None):\n",
        "        \"\"\"Scale the data with the Gauss Rank algorithm\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The data used to scale along the features axis.\n",
        "        copy : bool, optional (default: None)\n",
        "            Copy the input X or not.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'interp_func_')\n",
        "\n",
        "        copy = copy if copy is not None else self.copy\n",
        "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
        "\n",
        "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n",
        "        return X\n",
        "\n",
        "    def _transform(self, i, x):\n",
        "        return erfinv(self.interp_func_[i](x))\n",
        "\n",
        "    def inverse_transform(self, X, copy=None):\n",
        "        \"\"\"Scale back the data to the original representation\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape [n_samples, n_features]\n",
        "            The data used to scale along the features axis.\n",
        "        copy : bool, optional (default: None)\n",
        "            Copy the input X or not.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'interp_func_')\n",
        "\n",
        "        copy = copy if copy is not None else self.copy\n",
        "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
        "\n",
        "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n",
        "        return X\n",
        "\n",
        "    def _inverse_transform(self, i, x):\n",
        "        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, kind=self.interp_kind,\n",
        "                                   copy=self.interp_copy, fill_value=self.fill_value)\n",
        "        return inv_interp_func(erf(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_duplicates(x):\n",
        "        is_unique = np.zeros_like(x, dtype=bool)\n",
        "        is_unique[np.unique(x, return_index=True)[1]] = True\n",
        "        return x[is_unique]\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def rmspe_metric(y_true, y_pred):\n",
        "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "    return rmspe\n",
        "\n",
        "\n",
        "def rmspe_loss(y_true, y_pred):\n",
        "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
        "    return rmspe\n",
        "\n",
        "\n",
        "class RMSPE(Metric):\n",
        "    def __init__(self):\n",
        "        self._name = \"rmspe\"\n",
        "        self._maximize = False\n",
        "\n",
        "    def __call__(self, y_true, y_score):\n",
        "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
        "\n",
        "def RMSPELoss_Tabnet(y_pred, y_true):\n",
        "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
        "        super().__init__()\n",
        "        self.x_num = x_num\n",
        "        self.x_cat = x_cat\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_num)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
        "        else:\n",
        "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_num_dim: int,\n",
        "                 n_categories: List[int],\n",
        "                 dropout: float = 0.0,\n",
        "                 hidden: int = 50,\n",
        "                 emb_dim: int = 10,\n",
        "                 dropout_cat: float = 0.2,\n",
        "                 bn: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embs = nn.ModuleList([\n",
        "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
        "        self.cat_dim = emb_dim * len(n_categories)\n",
        "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
        "\n",
        "        if bn:\n",
        "            self.sequence = nn.Sequential(\n",
        "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.BatchNorm1d(hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden, hidden),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.BatchNorm1d(hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden, 1)\n",
        "            )\n",
        "        else:\n",
        "            self.sequence = nn.Sequential(\n",
        "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden, hidden),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
        "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
        "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
        "        x = self.sequence(x_all)\n",
        "        return torch.squeeze(x)\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_features: int,\n",
        "                 hidden_size: int,\n",
        "                 n_categories: List[int],\n",
        "                 emb_dim: int = 10,\n",
        "                 dropout_cat: float = 0.2,\n",
        "                 channel_1: int = 256,\n",
        "                 channel_2: int = 512,\n",
        "                 channel_3: int = 512,\n",
        "                 dropout_top: float = 0.1,\n",
        "                 dropout_mid: float = 0.3,\n",
        "                 dropout_bottom: float = 0.2,\n",
        "                 weight_norm: bool = True,\n",
        "                 two_stage: bool = True,\n",
        "                 celu: bool = True,\n",
        "                 kernel1: int = 5,\n",
        "                 leaky_relu: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        num_targets = 1\n",
        "\n",
        "        cha_1_reshape = int(hidden_size / channel_1)\n",
        "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
        "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
        "\n",
        "        self.cat_dim = emb_dim * len(n_categories)\n",
        "        self.cha_1 = channel_1\n",
        "        self.cha_2 = channel_2\n",
        "        self.cha_3 = channel_3\n",
        "        self.cha_1_reshape = cha_1_reshape\n",
        "        self.cha_po_1 = cha_po_1\n",
        "        self.cha_po_2 = cha_po_2\n",
        "        self.two_stage = two_stage\n",
        "\n",
        "        self.expand = nn.Sequential(\n",
        "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
        "            nn.Dropout(dropout_top),\n",
        "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
        "            nn.CELU(0.06) if celu else nn.ReLU()\n",
        "        )\n",
        "\n",
        "        def _norm(layer, dim=None):\n",
        "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.BatchNorm1d(channel_1),\n",
        "            nn.Dropout(dropout_top),\n",
        "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
        "            nn.BatchNorm1d(channel_2),\n",
        "            nn.Dropout(dropout_top),\n",
        "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        if self.two_stage:\n",
        "            self.conv2 = nn.Sequential(\n",
        "                nn.BatchNorm1d(channel_2),\n",
        "                nn.Dropout(dropout_mid),\n",
        "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(channel_2),\n",
        "                nn.Dropout(dropout_bottom),\n",
        "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.flt = nn.Flatten()\n",
        "\n",
        "        if leaky_relu:\n",
        "            self.dense = nn.Sequential(\n",
        "                nn.BatchNorm1d(cha_po_2),\n",
        "                nn.Dropout(dropout_bottom),\n",
        "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
        "                nn.LeakyReLU()\n",
        "            )\n",
        "        else:\n",
        "            self.dense = nn.Sequential(\n",
        "                nn.BatchNorm1d(cha_po_2),\n",
        "                nn.Dropout(dropout_bottom),\n",
        "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
        "            )\n",
        "\n",
        "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
        "        self.cat_dim = emb_dim * len(n_categories)\n",
        "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
        "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
        "        x = torch.cat([x_num, x_cat_emb], 1)\n",
        "\n",
        "        x = self.expand(x)\n",
        "\n",
        "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        if self.two_stage:\n",
        "            x = self.conv2(x) * x\n",
        "\n",
        "        x = self.max_po_c2(x)\n",
        "        x = self.flt(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return torch.squeeze(x)\n",
        "\n",
        "\n",
        "def preprocess_nn(\n",
        "        X: pd.DataFrame,\n",
        "        scaler: Optional[StandardScaler] = None,\n",
        "        scaler_type: str = 'standard',\n",
        "        n_pca: int = -1,\n",
        "        na_cols: bool = True):\n",
        "    if na_cols:\n",
        "        #for c in X.columns:\n",
        "        for c in null_check_cols:\n",
        "            if c in X.columns:\n",
        "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
        "\n",
        "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    X_num = X[num_cols].values.astype(np.float32)\n",
        "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
        "\n",
        "    def _pca(X_num_):\n",
        "        if n_pca > 0:\n",
        "            pca = PCA(n_components=n_pca, random_state=0)\n",
        "            return pca.fit_transform(X_num)\n",
        "        return X_num\n",
        "\n",
        "    if scaler is None:\n",
        "        if scaler_type == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif scaler_type == 'gauss':\n",
        "            scaler = GaussRankScaler()\n",
        "            X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
        "        X_num = scaler.fit_transform(X_num)\n",
        "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
        "        return _pca(X_num), X_cat, cat_cols, scaler\n",
        "    else:\n",
        "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
        "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
        "        return _pca(X_num), X_cat, cat_cols\n",
        "\n",
        "\n",
        "def train_epoch(data_loader: DataLoader,\n",
        "                model: nn.Module,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                device,\n",
        "                clip_grad: float = 1.5):\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    step = 0\n",
        "\n",
        "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
        "        batch_size = x_num.size(0)\n",
        "        x_num = x_num.to(device, dtype=torch.float)\n",
        "        x_cat = x_cat.to(device)\n",
        "        y = y.to(device, dtype=torch.float)\n",
        "\n",
        "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
        "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def evaluate(data_loader: DataLoader, model, device):\n",
        "    model.eval()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    final_targets = []\n",
        "    final_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
        "            batch_size = x_num.size(0)\n",
        "            x_num = x_num.to(device, dtype=torch.float)\n",
        "            x_cat = x_cat.to(device)\n",
        "            y = y.to(device, dtype=torch.float)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = model(x_num, x_cat)\n",
        "\n",
        "            loss = rmspe_loss(y, output)\n",
        "            # record loss\n",
        "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
        "\n",
        "            targets = y.detach().cpu().numpy()\n",
        "            output = output.detach().cpu().numpy()\n",
        "\n",
        "            final_targets.append(targets)\n",
        "            final_outputs.append(output)\n",
        "\n",
        "    final_targets = np.concatenate(final_targets)\n",
        "    final_outputs = np.concatenate(final_outputs)\n",
        "\n",
        "    try:\n",
        "        metric = rmspe_metric(final_targets, final_outputs)\n",
        "    except:\n",
        "        metric = None\n",
        "\n",
        "    return final_outputs, final_targets, losses.avg, metric\n",
        "\n",
        "\n",
        "def predict_nn(X: pd.DataFrame,\n",
        "               model: Union[List[MLP], MLP],\n",
        "               scaler: StandardScaler,\n",
        "               device,\n",
        "               ensemble_method='mean'):\n",
        "    if not isinstance(model, list):\n",
        "        model = [model]\n",
        "\n",
        "    for m in model:\n",
        "        m.eval()\n",
        "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
        "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                               batch_size=512,\n",
        "                                               shuffle=False,\n",
        "                                               num_workers=4)\n",
        "\n",
        "    final_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
        "            x_num = x_num.to(device, dtype=torch.float)\n",
        "            x_cat = x_cat.to(device)\n",
        "\n",
        "            outputs = []\n",
        "            with torch.no_grad():\n",
        "                for m in model:\n",
        "                    output = m(x_num, x_cat)\n",
        "                    outputs.append(output.detach().cpu().numpy())\n",
        "\n",
        "            if ensemble_method == 'median':\n",
        "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
        "            else:\n",
        "                pred = np.array(outputs).mean(axis=0)\n",
        "            final_outputs.append(pred)\n",
        "\n",
        "    final_outputs = np.concatenate(final_outputs)\n",
        "    return final_outputs\n",
        "\n",
        "\n",
        "def train_tabnet(X: pd.DataFrame,\n",
        "                 y: pd.DataFrame,\n",
        "                 folds: List[Tuple],\n",
        "                 batch_size: int = 1024,\n",
        "                 lr: float = 1e-3,\n",
        "                 model_path: str = 'fold_{}.pth',\n",
        "                 scaler_type: str = 'standard',\n",
        "                 output_dir: str = 'artifacts',\n",
        "                 epochs: int = 250,\n",
        "                 seed: int = 42,\n",
        "                 n_pca: int = -1,\n",
        "                 na_cols: bool = True,\n",
        "                 patience: int = 10,\n",
        "                 factor: float = 0.5):\n",
        "    seed_everything(seed)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    y = y.values.astype(np.float32)\n",
        "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
        "\n",
        "    best_losses = []\n",
        "    best_predictions = []\n",
        "\n",
        "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
        "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
        "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
        "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
        "        y_tr = y_tr.reshape(-1,1)\n",
        "        y_va = y_va.reshape(-1,1)\n",
        "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
        "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
        "\n",
        "        cat_idxs = [0]\n",
        "        cat_dims = [128]\n",
        "        model = TabNetRegressor(\n",
        "            cat_idxs=cat_idxs,\n",
        "            cat_dims=cat_dims,\n",
        "            cat_emb_dim=1,\n",
        "            n_d=16,\n",
        "            n_a=16,\n",
        "            n_steps=2,\n",
        "            gamma=2,\n",
        "            n_independent=2,\n",
        "            n_shared=2,\n",
        "            lambda_sparse=8,\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params={'lr': lr},\n",
        "            mask_type=\"entmax\",\n",
        "            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "            scheduler_params={'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True},\n",
        "            seed=seed,\n",
        "            verbose=10\n",
        "            #device_name=device,\n",
        "            #clip_value=1.5\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
        "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
        "\n",
        "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
        "        model.save_model(path)\n",
        "\n",
        "        predicted = model.predict(X_va)\n",
        "\n",
        "        rmspe = rmspe_metric(y_va, predicted)\n",
        "        best_losses.append(rmspe)\n",
        "        best_predictions.append(predicted)\n",
        "\n",
        "    return best_losses, best_predictions, scaler\n",
        "\n",
        "\n",
        "def train_nn(X: pd.DataFrame,\n",
        "             y: pd.DataFrame,\n",
        "             folds: List[Tuple],\n",
        "             device,\n",
        "             emb_dim: int = 25,\n",
        "             batch_size: int = 1024,\n",
        "             model_type: str = 'mlp',\n",
        "             mlp_dropout: float = 0.0,\n",
        "             mlp_hidden: int = 64,\n",
        "             mlp_bn: bool = False,\n",
        "             cnn_hidden: int = 64,\n",
        "             cnn_channel1: int = 32,\n",
        "             cnn_channel2: int = 32,\n",
        "             cnn_channel3: int = 32,\n",
        "             cnn_kernel1: int = 5,\n",
        "             cnn_celu: bool = False,\n",
        "             cnn_weight_norm: bool = False,\n",
        "             dropout_emb: bool = 0.0,\n",
        "             lr: float = 1e-3,\n",
        "             weight_decay: float = 0.0,\n",
        "             model_path: str = 'fold_{}.pth',\n",
        "             scaler_type: str = 'standard',\n",
        "             output_dir: str = 'artifacts',\n",
        "             scheduler_type: str = 'onecycle',\n",
        "             optimizer_type: str = 'adam',\n",
        "             max_lr: float = 0.01,\n",
        "             epochs: int = 30,\n",
        "             seed: int = 42,\n",
        "             n_pca: int = -1,\n",
        "             batch_double_freq: int = 50,\n",
        "             cnn_dropout: float = 0.1,\n",
        "             na_cols: bool = True,\n",
        "             cnn_leaky_relu: bool = False,\n",
        "             patience: int = 8,\n",
        "             factor: float = 0.5):\n",
        "    seed_everything(seed)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    y = y.values.astype(np.float32)\n",
        "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
        "\n",
        "    best_losses = []\n",
        "    best_predictions = []\n",
        "\n",
        "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
        "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
        "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
        "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
        "\n",
        "        cur_batch = batch_size\n",
        "        best_loss = 1e10\n",
        "        best_prediction = None\n",
        "\n",
        "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
        "\n",
        "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
        "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
        "                                                   num_workers=4)\n",
        "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
        "                                                   num_workers=4)\n",
        "\n",
        "        if model_type == 'mlp':\n",
        "            model = MLP(X_tr.shape[1],\n",
        "                        n_categories=[128],\n",
        "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
        "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
        "        elif model_type == 'cnn':\n",
        "            model = CNN(X_tr.shape[1],\n",
        "                        hidden_size=cnn_hidden,\n",
        "                        n_categories=[128],\n",
        "                        emb_dim=emb_dim,\n",
        "                        dropout_cat=dropout_emb,\n",
        "                        channel_1=cnn_channel1,\n",
        "                        channel_2=cnn_channel2,\n",
        "                        channel_3=cnn_channel3,\n",
        "                        two_stage=False,\n",
        "                        kernel1=cnn_kernel1,\n",
        "                        celu=cnn_celu,\n",
        "                        dropout_top=cnn_dropout,\n",
        "                        dropout_mid=cnn_dropout,\n",
        "                        dropout_bottom=cnn_dropout,\n",
        "                        weight_norm=cnn_weight_norm,\n",
        "                        leaky_relu=cnn_leaky_relu)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "        model = model.to(device)\n",
        "\n",
        "        if optimizer_type == 'adamw':\n",
        "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        elif optimizer_type == 'adam':\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        scheduler = epoch_scheduler = None\n",
        "        if scheduler_type == 'onecycle':\n",
        "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
        "                                                            max_lr=max_lr, epochs=epochs,\n",
        "                                                            steps_per_epoch=len(train_loader))\n",
        "        elif scheduler_type == 'reduce':\n",
        "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
        "                                                                         mode='min',\n",
        "                                                                         min_lr=1e-7,\n",
        "                                                                         patience=patience,\n",
        "                                                                         verbose=True,\n",
        "                                                                         factor=factor)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
        "                cur_batch = cur_batch * 2\n",
        "                print(f'batch: {cur_batch}')\n",
        "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                           batch_size=cur_batch,\n",
        "                                                           shuffle=True,\n",
        "                                                           num_workers=4)\n",
        "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
        "            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
        "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
        "\n",
        "            if epoch_scheduler is not None:\n",
        "                epoch_scheduler.step(rmspe)\n",
        "\n",
        "            if rmspe < best_loss:\n",
        "                print(f'new best:{rmspe}')\n",
        "                best_loss = rmspe\n",
        "                best_prediction = predictions\n",
        "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
        "\n",
        "        best_predictions.append(best_prediction)\n",
        "        best_losses.append(best_loss)\n",
        "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
        "        if scheduler is not None:\n",
        "            del scheduler\n",
        "        gc.collect()\n",
        "\n",
        "    return best_losses, best_predictions, scaler\n"
      ],
      "id": "a33d5397",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyx8uzwBTimh",
        "outputId": "8da621c2-00d6-42bc-9118-fc7aae6d1761"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "id": "qyx8uzwBTimh",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8da3112",
        "outputId": "6ca15f19-50f3-4e59-f1d0-411edf3b3ee1"
      },
      "source": [
        "!pip install optuna"
      ],
      "id": "c8da3112",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.2)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.9.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.23)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.2.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.6.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.2.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.2.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.4.0)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWb_JlDBT3ja",
        "outputId": "c4974a2f-86d7-4a32-b642-e9c6531d38e6"
      },
      "source": [
        "import optuna\n",
        "import logging\n",
        "\n",
        "def logging_callback(study, frozen_trial):\n",
        "    print(f\"**best**: {study.best_trial.params}\")\n",
        "\n",
        "def objective(trial):\n",
        "    gc.collect()\n",
        "    df_train, folds = prepare_df()\n",
        "    batch_size = trial.suggest_int('batch_size', 512, 2048, step=256)\n",
        "\n",
        "    #cnn_dropout = trial.suggest_float('cnn_dropout', 0.0, 0.2, step=0.1)\n",
        "    #mlp_dropout = trial.suggest_float('mlp_dropout', 0.0, 0.3, step=0.1)\n",
        "    #emb_dim = trial.suggest_int('emb_dim', 3, 40)\n",
        "    lr = 2e-2#trial.suggest_loguniform('lr', 3e-4, 4e-2)\n",
        "    #max_lr = trial.suggest_loguniform('max_lr', lr, 0.02)\n",
        "    #scaler_type = trial.suggest_categorical('scaler_type', ['standard', 'gauss'])\n",
        "    #mlp_bn = trial.suggest_categorical('mlp_bn', [False, True])\n",
        "    #epochs = trial.suggest_int('epochs', 20, 50, step=10)\n",
        "    #optimizer_type = trial.suggest_categorical('optimizer', ['adam', 'adamw'])\n",
        "    #cnn_weight_norm = trial.suggest_categorical('cnn_weight_norm', [False, True])\n",
        "\n",
        "    patience = trial.suggest_int('patience', 5, 15, step=5)\n",
        "    factor = trial.suggest_float('factor', 0.1, 0.6, step=0.1)\n",
        "\n",
        "    print(f'lr: {lr}')\n",
        "\n",
        "    try:\n",
        "      loss, _, _ = train_tabnet(df_train.drop('target', axis=1), \n",
        "          df_train['target'], \n",
        "          [folds[3]], \n",
        "          batch_size=batch_size,\n",
        "          epochs=50, #epochs,\n",
        "          lr=lr,\n",
        "          patience=50,\n",
        "          factor=0.5)\n",
        "    except:\n",
        "      logger.error(\"ERROR\")\n",
        "      loss = [10]\n",
        "      raise\n",
        "    \n",
        "    del df_train, folds\n",
        "    \n",
        "    return np.mean(loss)\n",
        "    \n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "logger.setLevel(logging.INFO)  # Setup the root logger.\n",
        "logger.addHandler(logging.FileHandler(\"optuna_tabnet_fold3.log\", mode=\"a\"))\n",
        "logger.addHandler(logging.StreamHandler())\n",
        "\n",
        "optuna.logging.enable_propagation()  # Propagate logs to the root logger.\n",
        "optuna.logging.disable_default_handler()  # Stop showing logs in sys.stderr.\n",
        "\n",
        "study = optuna.create_study()\n",
        "\n",
        "study.optimize(objective, n_trials=200, callbacks=[logging_callback])"
      ],
      "id": "qWb_JlDBT3ja",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new study created in memory with name: no-name-4b9903f9-40a7-43d9-8801-808bec1b019e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "folds0: train=257362, valid=42882\n",
            "folds1: train=300244, valid=42896\n",
            "folds2: train=343140, valid=42896\n",
            "folds3: train=386036, valid=42896\n",
            "[make folds]  2.010sec\n",
            "lr: 0.02\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 173.3325| val_0_rmspe: 17.388029098510742|  0:00:05s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgHneuJqfUiO"
      },
      "source": [
        "study"
      ],
      "id": "cgHneuJqfUiO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcKVVDjWRnr"
      },
      "source": [
        "from optuna.visualization import *\n",
        "\n",
        "plot_parallel_coordinate(study)"
      ],
      "id": "_BcKVVDjWRnr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4viE_g82rydX"
      },
      "source": [
        "plot_contour(study, ['lr', 'max_lr', 'mlp_hidden'])"
      ],
      "id": "4viE_g82rydX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbg54EpKsCOe"
      },
      "source": [
        "plot_contour(study, ['lr', 'epochs'])"
      ],
      "id": "Xbg54EpKsCOe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bwUuJJxsN1O"
      },
      "source": [
        "plot_param_importances(study)"
      ],
      "id": "0bwUuJJxsN1O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf8mlBnP6cAc"
      },
      "source": [
        "print(study.best_trial.value)"
      ],
      "id": "sf8mlBnP6cAc",
      "execution_count": null,
      "outputs": []
    }
  ]
}