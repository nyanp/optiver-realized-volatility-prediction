{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7006eaf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:01:00.680276Z",
     "iopub.status.busy": "2021-09-23T13:01:00.679274Z",
     "iopub.status.idle": "2021-09-23T13:01:04.540878Z",
     "shell.execute_reply": "2021-09-23T13:01:04.539868Z",
     "shell.execute_reply.started": "2021-09-22T12:58:06.940993Z"
    },
    "papermill": {
     "duration": 3.907865,
     "end_time": "2021-09-23T13:01:04.541048",
     "exception": false,
     "start_time": "2021-09-23T13:01:00.633183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from contextlib import contextmanager\n",
    "from enum import Enum\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from numba import jit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = '../input'\n",
    "\n",
    "USE_PRECOMPUTE_FEATURES = True\n",
    "PREDICT_CNN = False\n",
    "PREDICT_MLP = False\n",
    "PREDICT_GBDT = False\n",
    "PREDICT_TABNET = True\n",
    "IS_1ST_STAGE = False\n",
    "NN_VALID_TH = 0.19\n",
    "\n",
    "ENSEMBLE_METHOD = 'mean'\n",
    "NN_NUM_MODELS = 5\n",
    "TABNET_NUM_MODELS = 1\n",
    "SHORTCUT_NN_IN_1ST_STAGE = True\n",
    "GBDT_NUM_MODELS = 3\n",
    "GBDT_LR = 0.02  # 0.1\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}sec')\n",
    "\n",
    "\n",
    "class DataBlock(Enum):\n",
    "    TRAIN = 1\n",
    "    TEST = 2\n",
    "    BOTH = 3\n",
    "\n",
    "    \n",
    "def load_stock_data(stock_id, directory):\n",
    "    return pd.read_parquet(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', directory, f'stock_id={stock_id}'))\n",
    "    \n",
    "def load_data(stock_id, stem, block):\n",
    "    if block == DataBlock.TRAIN:\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')\n",
    "    elif block == DataBlock.TEST:\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, DataBlock.TRAIN),\n",
    "            load_data(stock_id, stem, DataBlock.TEST)\n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "def load_book(stock_id, block=DataBlock.TRAIN):\n",
    "    return load_data(stock_id, 'book', block)\n",
    "\n",
    "def load_trade(stock_id, block=DataBlock.TRAIN):\n",
    "    return load_data(stock_id, 'trade', block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603a4a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:01:04.589967Z",
     "iopub.status.busy": "2021-09-23T13:01:04.589413Z",
     "iopub.status.idle": "2021-09-23T13:01:31.402074Z",
     "shell.execute_reply": "2021-09-23T13:01:31.401498Z",
     "shell.execute_reply.started": "2021-09-23T12:00:51.427707Z"
    },
    "papermill": {
     "duration": 26.838359,
     "end_time": "2021-09-23T13:01:31.402221",
     "exception": false,
     "start_time": "2021-09-23T13:01:04.563862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528017df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:01:31.451824Z",
     "iopub.status.busy": "2021-09-23T13:01:31.451073Z",
     "iopub.status.idle": "2021-09-23T13:01:31.453728Z",
     "shell.execute_reply": "2021-09-23T13:01:31.453320Z",
     "shell.execute_reply.started": "2021-09-22T12:58:10.034709Z"
    },
    "papermill": {
     "duration": 0.029304,
     "end_time": "2021-09-23T13:01:31.453835",
     "exception": false,
     "start_time": "2021-09-23T13:01:31.424531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def in_colab() -> bool:\n",
    "    return 'google.colab' in sys.modules\n",
    "\n",
    "def in_kaggle() -> bool:\n",
    "    return 'kaggle_web_client' in sys.modules\n",
    "\n",
    "if in_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aeacfd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:01:31.502388Z",
     "iopub.status.busy": "2021-09-23T13:01:31.501862Z",
     "iopub.status.idle": "2021-09-23T13:01:31.790774Z",
     "shell.execute_reply": "2021-09-23T13:01:31.789583Z",
     "shell.execute_reply.started": "2021-09-22T12:58:10.042967Z"
    },
    "papermill": {
     "duration": 0.314764,
     "end_time": "2021-09-23T13:01:31.790905",
     "exception": false,
     "start_time": "2021-09-23T13:01:31.476141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'train.csv'))\n",
    "stock_ids = set(train['stock_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70911ce1",
   "metadata": {
    "papermill": {
     "duration": 0.021744,
     "end_time": "2021-09-23T13:01:31.835227",
     "exception": false,
     "start_time": "2021-09-23T13:01:31.813483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d50df53",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-09-23T13:01:31.919509Z",
     "iopub.status.busy": "2021-09-23T13:01:31.908376Z",
     "iopub.status.idle": "2021-09-23T13:01:42.243580Z",
     "shell.execute_reply": "2021-09-23T13:01:42.243112Z",
     "shell.execute_reply.started": "2021-09-22T12:58:10.333189Z"
    },
    "papermill": {
     "duration": 10.386673,
     "end_time": "2021-09-23T13:01:42.243748",
     "exception": false,
     "start_time": "2021-09-23T13:01:31.857075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load feather]  6.607sec\n",
      "[books]  1.139sec\n",
      "[trades]  0.315sec\n",
      "[extra features]  0.036sec\n",
      "[books(v2)]  0.021sec\n",
      "(428932, 216)\n",
      "(3, 216)\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def make_book_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    book['wap2'] = calc_wap2(book)\n",
    "    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n",
    "    # Calculate spread\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n",
    "    \n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n",
    "    agg.columns = flatten_name('book', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "    \n",
    "    for time in [450, 300, 150]:\n",
    "        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'book_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_trade_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    # Dict for aggregations\n",
    "    features = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':['count'],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        price_list = prices.loc[tid].values.flatten()\n",
    "        price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "        ticks[tid] = price_diff[0]\n",
    "        \n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    #book['tick_size'] = book['time_id'].map(ticks)\n",
    "    \n",
    "    # https://www.kaggle.com/lucasmorin/volatility-maximum-likelihood-estimation/comments#1495387\n",
    "    #book['log_wap'] = np.log(calc_wap1(book))\n",
    "    #book['log_return'] = book.groupby('time_id')['log_wap'].diff()\n",
    "    #book['dt'] = book.groupby('time_id')['seconds_in_bucket'].diff()\n",
    "    #book['inv_dt'] = 1 / book['dt']\n",
    "    #book['estimated_volatility'] = np.power(book['log_return'] / np.sqrt(book['dt']), 2)\n",
    "    #agg = book.groupby('time_id')['estimated_volatility'].sum()\n",
    "    #dst['book.estimated_volatility'] = dst['time_id'].map(agg)\n",
    "    #dst['book.estimated_volatility'] = np.sqrt(dst['book.estimated_volatility'])\n",
    "\n",
    "    #book['log_return_df2'] = book.groupby(['time_id'])['wap1'].apply(log_return_df2)\n",
    "    #book['price_spread_tick'] = (book['ask_price1'] - book['bid_price1']) / book['tick_size']\n",
    "    #agg = book.groupby('time_id')['inv_dt'].mean()\n",
    "    #dst['book.inv_dt.mean'] = dst['time_id'].map(agg)\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "def make_trade_feature_v2(stock_id, block = DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    # Dict for aggregations\n",
    "    features = {\n",
    "        'size':[np.mean],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "def make_features(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "        #df = make_extra_features(df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "    #with timer('trades(v2)'):\n",
    "    #    trades = Parallel(n_jobs=-1)(delayed(make_trade_feature_v2)(i, block) for i in stock_ids)\n",
    "    #    trade_v2 = pd.concat(trades)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d\n",
    "    #return pd.merge(d, trade_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    \n",
    "if USE_PRECOMPUTE_FEATURES:\n",
    "    with timer('load feather'):\n",
    "        df = pd.read_feather(os.path.join(DATA_DIR, 'optiver-df2', 'features_v2.f'))\n",
    "else:\n",
    "    df = make_features(train, DataBlock.TRAIN)\n",
    "    # v2\n",
    "    df = make_features_v2(df, DataBlock.TRAIN)\n",
    "\n",
    "df.to_feather('features_v2.f')\n",
    "\n",
    "test = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'test.csv'))\n",
    "if len(test) == 3:\n",
    "    IS_1ST_STAGE = True\n",
    "\n",
    "test_df = make_features(test, DataBlock.TEST)\n",
    "test_df = make_features_v2(test_df, DataBlock.TEST)\n",
    "\n",
    "print(df.shape)\n",
    "print(test_df.shape)\n",
    "df = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)\n",
    "\n",
    "df['trade.tau'] = np.sqrt(1 / df['trade.seconds_in_bucket.count'])\n",
    "df['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\n",
    "df['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\n",
    "#df['book.log_return1.realized_volatility.sq'] = np.power(df['book.log_return1.realized_volatility'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3fed35",
   "metadata": {
    "papermill": {
     "duration": 0.022518,
     "end_time": "2021-09-23T13:01:42.289630",
     "exception": false,
     "start_time": "2021-09-23T13:01:42.267112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Nearest-Neighbor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caedbf7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:01:42.385448Z",
     "iopub.status.busy": "2021-09-23T13:01:42.384505Z",
     "iopub.status.idle": "2021-09-23T13:08:29.585149Z",
     "shell.execute_reply": "2021-09-23T13:08:29.585684Z",
     "shell.execute_reply.started": "2021-09-22T12:58:26.234732Z"
    },
    "papermill": {
     "duration": 407.257481,
     "end_time": "2021-09-23T13:08:29.585893",
     "exception": false,
     "start_time": "2021-09-23T13:01:42.328412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[knn fit]  407.167sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "N_NEIGHBORS_MAX = 80\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, pivot, p, metric='minkowski', metric_params=None):\n",
    "        nn = NearestNeighbors(n_neighbors=N_NEIGHBORS_MAX, p=p, metric=metric, metric_params=metric_params)\n",
    "        nn.fit(pivot)\n",
    "        self.distances, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = df[['stock_id', 'time_id']].copy()\n",
    "    df_pv['price'] = 0.01 / df['tick_size']\n",
    "    df_pv['vol'] = df['book.log_return1.realized_volatility']\n",
    "    df_pv['trade.tau'] = df['trade.tau']\n",
    "    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n",
    "\n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'price')\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    \n",
    "    k_neighbors_p2 = Neighbors(pivot, 2, metric='canberra')\n",
    "    k_neighbors_p1 = Neighbors(pivot, 2, metric='mahalanobis', metric_params={'V':np.cov(pivot.values.T)})\n",
    "    k_neighbors_stock = Neighbors(minmax_scale(pivot.transpose()), 1)\n",
    "\n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n",
    "\n",
    "    #pivot = pd.concat([df_pv.pivot('time_id', 'stock_id', 'vol'), df_pv.pivot('time_id', 'stock_id', 'trade.tau')], axis=1).copy()\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    \n",
    "    k_neighbors_vol = Neighbors(pivot, 1)\n",
    "    k_neighbors_stock_vol = Neighbors(minmax_scale(pivot.transpose()), 1)\n",
    "    \n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'trade.size.sum')\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    k_neighbors_size = Neighbors(pivot, 2, metric='mahalanobis', metric_params={'V':np.cov(pivot.values.T)})\n",
    "    k_neighbors_size_p2 = Neighbors(pivot, 2, metric='canberra')\n",
    "    k_neighbors_stock_size = Neighbors(minmax_scale(pivot.transpose()), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a3c3e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:08:29.647280Z",
     "iopub.status.busy": "2021-09-23T13:08:29.646495Z",
     "iopub.status.idle": "2021-09-23T13:08:29.650273Z",
     "shell.execute_reply": "2021-09-23T13:08:29.649872Z",
     "shell.execute_reply.started": "2021-09-22T13:06:17.150192Z"
    },
    "papermill": {
     "duration": 0.038343,
     "end_time": "2021-09-23T13:08:29.650387",
     "exception": false,
     "start_time": "2021-09-23T13:08:29.612044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_neighbors(df, k_neighbors, feature_col, n=5):\n",
    "    feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "    feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "    feature_pivot.head()\n",
    "    \n",
    "    neighbors = np.zeros((n, *feature_pivot.shape))\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors[i, :, :] += feature_pivot.values[k_neighbors[:, i], :]\n",
    "        \n",
    "    return feature_pivot, neighbors\n",
    "\n",
    "def make_neighbors_stock(df, k_neighbors, feature_col, n=5):\n",
    "    feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "    feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "    feature_pivot.head()\n",
    "    \n",
    "    neighbors = np.zeros((n, *feature_pivot.shape))\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors[i, :, :] += feature_pivot.values[:, k_neighbors[:, i]]\n",
    "        \n",
    "    return feature_pivot, neighbors\n",
    "\n",
    "def make_nn_feature(df, neighbors, columns, index, n=5, agg=np.mean, postfix='', exclude_self=False, exact=False):\n",
    "    start = 1 if exclude_self else 0\n",
    "    \n",
    "    if exact:\n",
    "        pivot_aggs = pd.DataFrame(neighbors[n-1,:,:], columns=columns, index=index)\n",
    "    else:\n",
    "        pivot_aggs = pd.DataFrame(agg(neighbors[start:n,:,:], axis=0), columns=columns, index=index)\n",
    "    dst = pivot_aggs.unstack().reset_index()\n",
    "    dst.columns = ['stock_id', 'time_id', f'{feature_col}_cluster{n}{postfix}_{agg.__name__}']\n",
    "    return dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71817881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:08:29.729150Z",
     "iopub.status.busy": "2021-09-23T13:08:29.723357Z",
     "iopub.status.idle": "2021-09-23T13:09:48.496225Z",
     "shell.execute_reply": "2021-09-23T13:09:48.497585Z",
     "shell.execute_reply.started": "2021-09-22T13:06:17.162298Z"
    },
    "papermill": {
     "duration": 78.823184,
     "end_time": "2021-09-23T13:09:48.497865",
     "exception": false,
     "start_time": "2021-09-23T13:08:29.674681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428935, 219)\n",
      "(428935, 581)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def rank_data(a, axis=None):\n",
    "    return a[0] - np.min(a, axis=axis)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "df2 = df.copy()\n",
    "print(df2.shape)\n",
    "\n",
    "df2['real_price'] = 0.01 / df2['tick_size']\n",
    "del df2['tick_size']\n",
    "\n",
    "# relative rank\n",
    "df2['trade.order_count.mean'] = df2.groupby('time_id')['trade.order_count.mean'].rank()\n",
    "df2['book.total_volume.sum'] = df2.groupby('time_id')['book.total_volume.sum'].rank()\n",
    "df2['book.total_volume.mean'] = df2.groupby('time_id')['book.total_volume.mean'].rank()\n",
    "df2['book.total_volume.std'] = df2.groupby('time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "df2['trade.tau'] = df2.groupby('time_id')['trade.tau'].rank()\n",
    "\n",
    "#df2['trade_300.tau'] = df2.groupby('time_id')['trade_300.tau'].rank()\n",
    "#df2['book.total_volume.std'] = df2.groupby('time_id')['book.total_volume.std'].rank()\n",
    "#df2['trade.size.sum'] = df2.groupby('time_id')['trade.size.sum'].rank()\n",
    "\n",
    "for dt in [150, 300, 450]:\n",
    "    df2[f'book_{dt}.total_volume.sum'] = df2.groupby('time_id')[f'book_{dt}.total_volume.sum'].rank()\n",
    "    df2[f'book_{dt}.total_volume.mean'] = df2.groupby('time_id')[f'book_{dt}.total_volume.mean'].rank()\n",
    "    df2[f'book_{dt}.total_volume.std'] = df2.groupby('time_id')[f'book_{dt}.total_volume.std'].rank()\n",
    "    df2[f'trade_{dt}.order_count.mean'] = df2.groupby('time_id')[f'trade_{dt}.order_count.mean'].rank()\n",
    "\n",
    "#df2.groupby('time_id')['book.volume_imbalance.sum'].rank()\n",
    "#df2['tick_size'] = df2.groupby('time_id')['tick_size'].rank()\n",
    "\n",
    "feature_cols_stock = {\n",
    "    'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "\n",
    "    'trade.seconds_in_bucket.count': [np.mean],\n",
    "    'trade.tau': [np.mean],\n",
    "    'trade_150.tau': [np.mean],\n",
    "    'book.tau': [np.mean],\n",
    "    'trade.size.sum': [np.mean],\n",
    "    'book.seconds_in_bucket.count': [np.mean],\n",
    "    \n",
    "    #'trade.order_count.mean': [np.mean],\n",
    "    #'avg_time_vol': [np.mean],\n",
    "    #'trade_150.tau': [np.mean],\n",
    "    #'trade_450.tau': [np.mean],\n",
    "    #'book.total_volume.sum': [np.mean],\n",
    "    #'book.volume_imbalance.mean': [np.mean],\n",
    "}\n",
    "    \n",
    "feature_cols = {\n",
    "    'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "    #'book_150.log_return1.realized_volatility': [np.mean, np.min],\n",
    "    #'book_300.log_return1.realized_volatility': [np.mean, np.min],\n",
    "    #'book_450.log_return1.realized_volatility': [np.mean, np.min],\n",
    "    \n",
    "    'real_price': [np.max, np.mean, np.min],\n",
    "\n",
    "    'trade.seconds_in_bucket.count': [np.mean],\n",
    "    'trade.tau': [np.mean],\n",
    "    'trade.size.sum': [np.mean],\n",
    "    'book.seconds_in_bucket.count': [np.mean],\n",
    "    \n",
    "    'trade_150.tau_cluster20_sv_mean': [np.mean],  # \"volatilityの傾向が似ている20銘柄での直前300secの平均tau\" の、近い時刻での平均\n",
    "    'trade.size.sum_cluster20_sv_mean': [np.mean],\n",
    "    #'book.log_return1.realized_volatility.sq': [np.sum],\n",
    "    #'book.estimated_volatility': [np.mean],\n",
    "    #'trade.order_count.mean': [np.mean],\n",
    "    #'book.total_volume.sum': [np.mean],\n",
    "    #'book.volume_imbalance.mean': [np.mean],\n",
    "    #'avg_time_vol': [np.mean],\n",
    "    #'book.ask_spread.mean': [np.mean],\n",
    "    #'book.bid_spread.mean': [np.mean],\n",
    "}\n",
    "\n",
    "time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "ndf = None\n",
    "\n",
    "cols = []\n",
    "\n",
    "def _add_ndf(ndf, dst):\n",
    "    if ndf is None:\n",
    "        return dst\n",
    "    else:\n",
    "        ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "        return ndf\n",
    "\n",
    "# neighbor stock_id\n",
    "for feature_col in feature_cols_stock.keys():\n",
    "    feature_pivot, neighbors_stock = make_neighbors_stock(df2, k_neighbors_stock.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    _, neighbors_stock_vol = make_neighbors_stock(df2, k_neighbors_stock_vol.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    _, neighbors_stock_size = make_neighbors_stock(df2, k_neighbors_stock_size.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "\n",
    "    columns = feature_pivot.columns\n",
    "    index = feature_pivot.index\n",
    "\n",
    "    for agg in feature_cols_stock[feature_col]:\n",
    "        for n in stock_id_neighbor_sizes:\n",
    "            exclude_self = True\n",
    "            exact = False\n",
    "            dst = make_nn_feature(df2, neighbors_stock, columns, index, n=n, agg=agg, postfix='_s',\n",
    "                                 exclude_self=exclude_self, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "            dst = make_nn_feature(df2, neighbors_stock_vol, columns, index, n=n, agg=agg, postfix='_sv',\n",
    "                                 exclude_self=exclude_self, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "            #dst = make_nn_feature(df2, neighbors_stock_size, columns, index, n=n, agg=agg, postfix='_ssize',\n",
    "            #                     exclude_self=exclude_self)\n",
    "            #ndf = _add_ndf(ndf, dst)\n",
    "    del feature_pivot, neighbors_stock, neighbors_stock_vol\n",
    "\n",
    "df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "ndf = None\n",
    "\n",
    "# neighbor time_id\n",
    "for feature_col in feature_cols.keys():\n",
    "    feature_pivot, neighbors = make_neighbors(df2, k_neighbors_p2.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    _, neighbors_p1 = make_neighbors(df2, k_neighbors_p1.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    _, neighbors_vol = make_neighbors(df2, k_neighbors_vol.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    _, neighbors_size = make_neighbors(df2, k_neighbors_size.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    _, neighbors_size_p2 = make_neighbors(df2, k_neighbors_size_p2.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "    \n",
    "    columns = feature_pivot.columns\n",
    "    index = feature_pivot.index\n",
    "    \n",
    "    if 'volatility' in feature_col:\n",
    "        time_id_ns = time_id_neigbor_sizes_vol\n",
    "    else:\n",
    "        time_id_ns = time_id_neigbor_sizes\n",
    " \n",
    "    for agg in feature_cols[feature_col]:\n",
    "        for n in time_id_ns:\n",
    "            exclude_self = True #n >= 10\n",
    "            exclude_self2 = False\n",
    "            exact = False\n",
    "            \n",
    "            #if n <= 40:\n",
    "            dst = make_nn_feature(df2, neighbors, columns, index, n=n, agg=agg, postfix='_p2',\n",
    "                                  exclude_self=exclude_self, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "            dst = make_nn_feature(df2, neighbors_p1, columns, index, n=n, agg=agg, postfix='_p1',\n",
    "                                 exclude_self=exclude_self2, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "\n",
    "            dst = make_nn_feature(df2, neighbors_vol, columns, index, n=n, agg=agg, postfix='_v',\n",
    "                                 exclude_self=exclude_self2, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "            dst = make_nn_feature(df2, neighbors_size, columns, index, n=n, agg=agg, postfix='_size',\n",
    "                                 exclude_self=exclude_self2, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "            dst = make_nn_feature(df2, neighbors_size_p2, columns, index, n=n, agg=agg, postfix='_size_p2',\n",
    "                                 exclude_self=exclude_self2, exact=exact)\n",
    "            ndf = _add_ndf(ndf, dst)\n",
    "            cols.append(dst.columns[-1])\n",
    "            \n",
    "    del feature_pivot, neighbors, neighbors_p1, neighbors_vol, neighbors_size, neighbors_size_p2\n",
    "\n",
    "df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "# 株価そのものは特徴量に使わず、近傍のtime_idに対する相対評価のみ用いる\n",
    "\n",
    "for sz in time_id_neigbor_sizes:\n",
    "    df2[f'real_price_rankmin_{sz}'] = df2['real_price'] / df2[f\"real_price_cluster{sz}_p2_amin\"]\n",
    "    df2[f'real_price_rankmax_{sz}'] = df2['real_price'] / df2[f\"real_price_cluster{sz}_p2_amax\"]\n",
    "    df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"real_price_cluster{sz}_p2_mean\"]\n",
    "\n",
    "for sz in time_id_neigbor_sizes_vol:\n",
    "    df2[f'vol_rankmin_{sz}'] = df2['book.log_return1.realized_volatility'] / df2[f\"book.log_return1.realized_volatility_cluster{sz}_p2_amin\"]\n",
    "    df2[f'vol_rankmax_{sz}'] = df2['book.log_return1.realized_volatility'] / df2[f\"book.log_return1.realized_volatility_cluster{sz}_p2_amax\"]\n",
    "\n",
    "price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "for c in price_cols:\n",
    "    del df2[c]\n",
    "\n",
    "#df2['book.log_return1.realized_volatility_rank'] = df2.groupby('time_id')['book.log_return1.realized_volatility'].rank()\n",
    "#df2['book.log_return1.realized_volatility_cluster3_size_mean_rank'] = df2.groupby('time_id')['book.log_return1.realized_volatility_cluster3_size_mean'].rank()\n",
    "\n",
    "for sz in time_id_neigbor_sizes_vol:\n",
    "    tgt = f'book.log_return1.realized_volatility_cluster{sz}_p1_mean'\n",
    "    df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
    "    \n",
    "    \n",
    "# skew correction for NN\n",
    "cols_to_log = [\n",
    "    'trade.size.sum',\n",
    "    'trade_150.size.sum',\n",
    "    'trade_300.size.sum',\n",
    "    'trade_450.size.sum',\n",
    "    'volume_imbalance'\n",
    "]\n",
    "for c in df2.columns:\n",
    "    for check in cols_to_log:\n",
    "        if check in c:\n",
    "            df2[c] = np.log(df2[c]+1)\n",
    "            break\n",
    "\n",
    "print(df2.shape)\n",
    "df2.reset_index(drop=True).to_feather('optiver_df2.f')\n",
    "\n",
    "del ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c608282",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:09:48.568329Z",
     "iopub.status.busy": "2021-09-23T13:09:48.567535Z",
     "iopub.status.idle": "2021-09-23T13:09:49.926341Z",
     "shell.execute_reply": "2021-09-23T13:09:49.925375Z",
     "shell.execute_reply.started": "2021-09-22T13:08:05.329653Z"
    },
    "papermill": {
     "duration": 1.388034,
     "end_time": "2021-09-23T13:09:49.926490",
     "exception": false,
     "start_time": "2021-09-23T13:09:48.538456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 同じ銘柄で近い取引量の時のRVを平均する\n",
    "df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "df2['realized_volatility_roll3_by_book.total_volume.mean'] = df2.groupby('stock_id')['book.log_return1.realized_volatility'].rolling(3, center=True, min_periods=1).mean().reset_index().sort_values(by=['level_1'])['book.log_return1.realized_volatility'].values\n",
    "df2['realized_volatility_roll10_by_book.total_volume.mean'] = df2.groupby('stock_id')['book.log_return1.realized_volatility'].rolling(10, center=True, min_periods=1).mean().reset_index().sort_values(by=['level_1'])['book.log_return1.realized_volatility'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e19066",
   "metadata": {
    "papermill": {
     "duration": 0.139733,
     "end_time": "2021-09-23T13:09:50.091999",
     "exception": false,
     "start_time": "2021-09-23T13:09:49.952266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Stock-id Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2435caea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:09:50.172469Z",
     "iopub.status.busy": "2021-09-23T13:09:50.166745Z",
     "iopub.status.idle": "2021-09-23T13:09:53.116587Z",
     "shell.execute_reply": "2021-09-23T13:09:53.115808Z",
     "shell.execute_reply.started": "2021-09-22T13:08:06.921169Z"
    },
    "papermill": {
     "duration": 3.000227,
     "end_time": "2021-09-23T13:09:53.116731",
     "exception": false,
     "start_time": "2021-09-23T13:09:50.116504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_n = 3\n",
    "lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n",
    "stock_id_emb = pd.DataFrame(lda.fit_transform(pivot.transpose()), index=df_pv.pivot('time_id', 'stock_id', 'vol').columns)\n",
    "\n",
    "for i in range(lda_n):\n",
    "    df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d49c3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:09:53.168737Z",
     "iopub.status.busy": "2021-09-23T13:09:53.167855Z",
     "iopub.status.idle": "2021-09-23T13:09:55.274675Z",
     "shell.execute_reply": "2021-09-23T13:09:55.275171Z",
     "shell.execute_reply.started": "2021-09-22T13:08:09.962398Z"
    },
    "papermill": {
     "duration": 2.134946,
     "end_time": "2021-09-23T13:09:55.275345",
     "exception": false,
     "start_time": "2021-09-23T13:09:53.140399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df2[~df2.target.isnull()].copy()\n",
    "df_test = df2[df2.target.isnull()].copy()\n",
    "del df2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ef3c2",
   "metadata": {
    "papermill": {
     "duration": 3.346055,
     "end_time": "2021-09-23T13:09:58.654302",
     "exception": false,
     "start_time": "2021-09-23T13:09:55.308247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4790c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:09:58.729416Z",
     "iopub.status.busy": "2021-09-23T13:09:58.728781Z",
     "iopub.status.idle": "2021-09-23T13:09:58.793253Z",
     "shell.execute_reply": "2021-09-23T13:09:58.792795Z",
     "shell.execute_reply.started": "2021-09-22T13:08:11.89487Z"
    },
    "papermill": {
     "duration": 0.114538,
     "end_time": "2021-09-23T13:09:58.793362",
     "exception": false,
     "start_time": "2021-09-23T13:09:58.678824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "    \n",
    "\n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    # AMZN\n",
    "    plt.plot(dft['stock_id=61'])\n",
    "    plt.plot(dft['stock_id=37'])\n",
    "    plt.plot(dft['stock_id=113'])\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        df_files = pd.DataFrame(\n",
    "            {'book_path': glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet')}) \\\n",
    "            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n",
    "        df_target = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n",
    "        df_target = df_target.groupby('time_id').target.mean()\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    plt.plot(df_ordered['stock_id=61'])\n",
    "    \n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4c87ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:09:58.847979Z",
     "iopub.status.busy": "2021-09-23T13:09:58.847439Z",
     "iopub.status.idle": "2021-09-23T13:10:00.679256Z",
     "shell.execute_reply": "2021-09-23T13:10:00.679671Z",
     "shell.execute_reply.started": "2021-09-22T13:08:12.008144Z"
    },
    "papermill": {
     "duration": 1.862807,
     "end_time": "2021-09-23T13:10:00.679832",
     "exception": false,
     "start_time": "2021-09-23T13:09:58.817025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate order of time-id] 0.009sec\n",
      "folds0: train=257362, valid=42882\n",
      "folds1: train=300244, valid=42896\n",
      "folds2: train=343140, valid=42896\n",
      "folds3: train=386036, valid=42896\n",
      "[make folds] 1.817sec\n"
     ]
    }
   ],
   "source": [
    "with timer('calculate order of time-id'):\n",
    "    if USE_PRECOMPUTE_FEATURES:\n",
    "        timeid_order = pd.read_csv(os.path.join(DATA_DIR, 'optiver-time-id-ordered', 'time_id_order.csv'))\n",
    "    else:\n",
    "        timeid_order = reconstruct_time_id_order()\n",
    "\n",
    "with timer('make folds'):\n",
    "    timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
    "    df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
    "    df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "    folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n",
    "    time_id_orders = df_train['time_id_order']\n",
    "\n",
    "    folds = []\n",
    "    for i, border in enumerate(folds_border):\n",
    "        idx_train = np.where(time_id_orders < border)[0]\n",
    "        idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n",
    "        folds.append((idx_train, idx_valid))\n",
    "        \n",
    "        print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n",
    "        \n",
    "del df_train['time_id_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cf54a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:10:00.735586Z",
     "iopub.status.busy": "2021-09-23T13:10:00.734868Z",
     "iopub.status.idle": "2021-09-23T13:10:00.737837Z",
     "shell.execute_reply": "2021-09-23T13:10:00.738199Z",
     "shell.execute_reply.started": "2021-09-22T13:08:14.185216Z"
    },
    "papermill": {
     "duration": 0.034049,
     "end_time": "2021-09-23T13:10:00.738323",
     "exception": false,
     "start_time": "2021-09-23T13:10:00.704274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for dt in [3]:\n",
    "        roll = df_train.groupby('stock_id')['real_price'].rolling(dt).mean().reset_index().sort_values(by=['level_1'])['real_price'].values\n",
    "        df_train[f'real_price.back_mean_{dt}'] = roll10\n",
    "        df_train[f'real_price.forward_mean_{dt}'] = df_train.groupby('stock_id')[f'real_price.back_mean_{dt}'].shift(-dt)\n",
    "        df_train[f'real_price.back_mean_{dt}'] = df_train['real_price'] / df_train[f'real_price.back_mean_{dt}']\n",
    "        df_train[f'real_price.back_forward_{dt}'] = df_train['real_price'] / df_train[f'real_price.forward_mean_{dt}']\n",
    "        \n",
    "    del df_train['real_price']\n",
    "\n",
    "# 3/4foldで改善したが、fold1が大きく悪化。\n",
    "#df_train['ratio1'] = df_train['trade.log_return.realized_volatility'] / df_train['book.log_return1.realized_volatility']\n",
    "#df_train['ratio2'] = df_train['book.log_return1.realized_volatility_cluster3_p2_mean'] / df_train['book.log_return1.realized_volatility']\n",
    "#df_train['ratio3'] = df_train['book.log_return1.realized_volatility_cluster3_p1_mean'] / df_train['book.log_return1.realized_volatility']\n",
    "#df_train['ratio4'] = df_train['book_300.log_return1.realized_volatility'] / df_train['book.log_return1.realized_volatility']\n",
    "#df_train['ratio5'] = df_train['book.log_return1.realized_volatility_cluster3_p2_mean'] / df_train['book.log_return1.realized_volatility_cluster10_p2_mean']\n",
    "#df_train['ratio6'] = df_train['trade.tau'] / df_train['trade.tau_cluster3_p2_mean']\n",
    "\n",
    "#df_train['ratio7'] = df_train['trade.size.sum'] / df_train['trade.size.sum_cluster3_p2_mean']\n",
    "#df_train['ratio８'] = df_train['trade.size.sum'] / df_train['trade.size.sum_cluster5_p2_mean']\n",
    "\n",
    "# ばらつき大.\n",
    "#df_train['diff1'] = df_train['book.log_return1.realized_volatility'] - df_train['book.log_return1.realized_volatility_cluster3_p2_mean']\n",
    "#df_train['diff2'] = df_train['book.log_return1.realized_volatility_cluster3_p1_mean'] - df_train['book.log_return1.realized_volatility_cluster3_p2_mean']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6bfe5c",
   "metadata": {
    "papermill": {
     "duration": 0.02399,
     "end_time": "2021-09-23T13:10:00.786752",
     "exception": false,
     "start_time": "2021-09-23T13:10:00.762762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9d29ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:10:00.839487Z",
     "iopub.status.busy": "2021-09-23T13:10:00.838573Z",
     "iopub.status.idle": "2021-09-23T13:10:05.127763Z",
     "shell.execute_reply": "2021-09-23T13:10:05.128939Z"
    },
    "papermill": {
     "duration": 4.31793,
     "end_time": "2021-09-23T13:10:05.129194",
     "exception": false,
     "start_time": "2021-09-23T13:10:00.811264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 584)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # 平均値でソートする\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # 上位をプロットする\n",
    "    PLOT_TOP_N = 80\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "lr = GBDT_LR\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0,\n",
    "    'metric': '',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 5,\n",
    "    'min_data_in_leaf': 1000,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 128,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'learning_rate': lr\n",
    "}\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "        features = list(self.models[0].feature_name())\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            assert features == list(m.feature_name())\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "        for i, m in enumerate(self.models):\n",
    "            w = self.weights[i] if self.weights is not None else 1\n",
    "            predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "        return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "    def feature_name(self) -> List[str]:\n",
    "        return self.models[0].feature_name()\n",
    "\n",
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target']]\n",
    "    return df_src[cols]\n",
    "\n",
    "X = get_X(df_train)\n",
    "y = df_train['target']\n",
    "X.to_feather('X.f')\n",
    "df_train[['target']].to_feather('y.f')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "    ds = lgb.Dataset(X, y, weight=1/np.power(y, 2))\n",
    "\n",
    "    with timer('lgb.cv'):\n",
    "        ret = lgb.cv(params, ds, num_boost_round=8000, folds=folds, #cv,\n",
    "                     feval=feval_RMSPE, stratified=False, \n",
    "                     return_cvbooster=True, verbose_eval=20,\n",
    "                     early_stopping_rounds=int(40*0.1/lr))\n",
    "\n",
    "        # current best: 0.204666 (cluster groupkfold)\n",
    "        # current best: 0.20629 (time-series, 4folds, 10% valid)\n",
    "        print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n",
    "\n",
    "    best_iteration = len(ret['RMSPE-mean'])\n",
    "    for i in range(len(folds)):\n",
    "        y_pred = ret['cvbooster'].boosters[i].predict(X.iloc[folds[i][1]], num_iteration=best_iteration)\n",
    "        y_true = y.iloc[folds[i][1]]\n",
    "        print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n",
    "        \n",
    "        if i == len(folds) - 1:\n",
    "            np.save('fold3_pred_gbdt.npy', y_pred)\n",
    "\n",
    "    plot_importance(ret['cvbooster'], figsize=(10, 20))\n",
    "\n",
    "\n",
    "    boosters = []\n",
    "    with timer('retraining'):\n",
    "        for _ in range(GBDT_NUM_MODELS):\n",
    "            boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n",
    "\n",
    "    booster = EnsembleModel(boosters)\n",
    "    del ret\n",
    "\n",
    "    #del X, y, ds\n",
    "    del ds\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#[20]\tcv_agg's l2: 2.32685e-07 + 2.96471e-08\tcv_agg's RMSPE: 0.215317 + 0.0105853\n",
    "#[40]\tcv_agg's l2: 2.07503e-07 + 2.60842e-08\tcv_agg's RMSPE: 0.203468 + 0.0116355\n",
    "#[60]\tcv_agg's l2: 2.04544e-07 + 2.61008e-08\tcv_agg's RMSPE: 0.201977 + 0.0113603\n",
    "#[80]\tcv_agg's l2: 2.03579e-07 + 2.59835e-08\tcv_agg's RMSPE: 0.20149 + 0.0112293\n",
    "#[100]\tcv_agg's l2: 2.0297e-07 + 2.57662e-08\tcv_agg's RMSPE: 0.201195 + 0.0111581\n",
    "#[120]\tcv_agg's l2: 2.02843e-07 + 2.58755e-08\tcv_agg's RMSPE: 0.20112 + 0.0111222\n",
    "#[140]\tcv_agg's l2: 2.02751e-07 + 2.58503e-08\tcv_agg's RMSPE: 0.201075 + 0.011069\n",
    "#[160]\tcv_agg's l2: 2.02773e-07 + 2.59782e-08\tcv_agg's RMSPE: 0.201073 + 0.0110104\n",
    "#[180]\tcv_agg's l2: 2.02933e-07 + 2.6137e-08\tcv_agg's RMSPE: 0.201148 + 0.0110521\n",
    "# overall RMSPE: 0.201055\n",
    "# fold0 RMSPE: 0.20412414796779355\n",
    "# fold1 RMSPE: 0.2101286514142503\n",
    "# fold2 RMSPE: 0.20769188046340817\n",
    "# fold3 RMSPE: 0.1822814692175734"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7294d",
   "metadata": {
    "papermill": {
     "duration": 0.043738,
     "end_time": "2021-09-23T13:10:05.239273",
     "exception": false,
     "start_time": "2021-09-23T13:10:05.195535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f58aba",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-09-23T13:10:05.314803Z",
     "iopub.status.busy": "2021-09-23T13:10:05.298771Z",
     "iopub.status.idle": "2021-09-23T13:10:06.541162Z",
     "shell.execute_reply": "2021-09-23T13:10:06.540227Z"
    },
    "papermill": {
     "duration": 1.277308,
     "end_time": "2021-09-23T13:10:06.541313",
     "exception": false,
     "start_time": "2021-09-23T13:10:05.264005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import erf, erfinv\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "class GaussRankScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transform features by scaling each feature to a normal distribution.\n",
    "    Parameters\n",
    "        ----------\n",
    "        epsilon : float, optional, default 1e-4\n",
    "            A small amount added to the lower bound or subtracted\n",
    "            from the upper bound. This value prevents infinite number\n",
    "            from occurring when applying the inverse error function.\n",
    "        copy : boolean, optional, default True\n",
    "            If False, try to avoid a copy and do inplace scaling instead.\n",
    "            This is not guaranteed to always work inplace; e.g. if the data is\n",
    "            not a NumPy array, a copy may still be returned.\n",
    "        n_jobs : int or None, optional, default None\n",
    "            Number of jobs to run in parallel.\n",
    "            ``None`` means 1 and ``-1`` means using all processors.\n",
    "        interp_kind : str or int, optional, default 'linear'\n",
    "           Specifies the kind of interpolation as a string\n",
    "            ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
    "            'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n",
    "            refer to a spline interpolation of zeroth, first, second or third\n",
    "            order; 'previous' and 'next' simply return the previous or next value\n",
    "            of the point) or as an integer specifying the order of the spline\n",
    "            interpolator to use.\n",
    "        interp_copy : bool, optional, default False\n",
    "            If True, the interpolation function makes internal copies of x and y.\n",
    "            If False, references to `x` and `y` are used.\n",
    "        Attributes\n",
    "        ----------\n",
    "        interp_func_ : list\n",
    "            The interpolation function for each feature in the training set.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.copy = copy\n",
    "        self.interp_kind = interp_kind\n",
    "        self.interp_copy = interp_copy\n",
    "        self.fill_value = 'extrapolate'\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit interpolation function to link rank with original data for future scaling\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to fit interpolation function for later scaling along the features axis.\n",
    "        y\n",
    "            Ignored\n",
    "        \"\"\"\n",
    "        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit)(x) for x in X.T)\n",
    "        return self\n",
    "\n",
    "    def _fit(self, x):\n",
    "        x = self.drop_duplicates(x)\n",
    "        rank = np.argsort(np.argsort(x))\n",
    "        bound = 1.0 - self.epsilon\n",
    "        factor = np.max(rank) / 2.0 * bound\n",
    "        scaled_rank = np.clip(rank / factor - bound, -bound, bound)\n",
    "        return interp1d(\n",
    "            x, scaled_rank, kind=self.interp_kind, copy=self.interp_copy, fill_value=self.fill_value)\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"Scale the data with the Gauss Rank algorithm\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _transform(self, i, x):\n",
    "        return erfinv(self.interp_func_[i](x))\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        \"\"\"Scale back the data to the original representation\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _inverse_transform(self, i, x):\n",
    "        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, kind=self.interp_kind,\n",
    "                                   copy=self.interp_copy, fill_value=self.fill_value)\n",
    "        return inv_interp_func(erf(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_duplicates(x):\n",
    "        is_unique = np.zeros_like(x, dtype=bool)\n",
    "        is_unique[np.unique(x, return_index=True)[1]] = True\n",
    "        return x[is_unique]\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        if scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'gauss':\n",
    "            scaler = GaussRankScaler()\n",
    "            X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "\n",
    "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_num, X_cat], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine'):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = [0]\n",
    "        cat_dims = [128]\n",
    "\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=16,\n",
    "            n_a=16,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10\n",
    "            #device_name=device,\n",
    "            #clip_value=1.5\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        rmspe = rmspe_metric(y_va, predicted)\n",
    "        best_losses.append(rmspe)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 1e10\n",
    "        best_prediction = None\n",
    "\n",
    "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            model = MLP(X_tr.shape[1],\n",
    "                        n_categories=[128],\n",
    "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
    "        elif model_type == 'cnn':\n",
    "            model = CNN(X_tr.shape[1],\n",
    "                        hidden_size=cnn_hidden,\n",
    "                        n_categories=[128],\n",
    "                        emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb,\n",
    "                        channel_1=cnn_channel1,\n",
    "                        channel_2=cnn_channel2,\n",
    "                        channel_3=cnn_channel3,\n",
    "                        two_stage=False,\n",
    "                        kernel1=cnn_kernel1,\n",
    "                        celu=cnn_celu,\n",
    "                        dropout_top=cnn_dropout,\n",
    "                        dropout_mid=cnn_dropout,\n",
    "                        dropout_bottom=cnn_dropout,\n",
    "                        weight_norm=cnn_weight_norm,\n",
    "                        leaky_relu=cnn_leaky_relu)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = model.to(device)\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='min',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=4)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(rmspe)\n",
    "\n",
    "            if rmspe < best_loss:\n",
    "                print(f'new best:{rmspe}')\n",
    "                best_loss = rmspe\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4704cecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:10:07.920559Z",
     "iopub.status.busy": "2021-09-23T13:10:07.911803Z",
     "iopub.status.idle": "2021-09-23T13:10:08.020235Z",
     "shell.execute_reply": "2021-09-23T13:10:08.019798Z"
    },
    "papermill": {
     "duration": 1.4539,
     "end_time": "2021-09-23T13:10:08.020362",
     "exception": false,
     "start_time": "2021-09-23T13:10:06.566462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n",
      "|                        X|1336552272|\n",
      "|                       df| 646834140|\n",
      "|                    df_pv|  20589040|\n",
      "|                 df_train|1343415184|\n",
      "|                      dst|  10303264|\n",
      "|                    index|    162832|\n",
      "|     neighbors_stock_size| 274749568|\n",
      "|                    pivot|   3434528|\n",
      "|           time_id_orders|   3431616|\n",
      "|             timeid_order|     61440|\n",
      "|                    train|  10294528|\n",
      "|                        y|   3431616|\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #ここだけアレンジ\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c45f1d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:10:08.267617Z",
     "iopub.status.busy": "2021-09-23T13:10:08.084386Z",
     "iopub.status.idle": "2021-09-23T13:49:37.791679Z",
     "shell.execute_reply": "2021-09-23T13:49:37.791037Z"
    },
    "papermill": {
     "duration": 2369.74572,
     "end_time": "2021-09-23T13:49:37.791845",
     "exception": false,
     "start_time": "2021-09-23T13:10:08.046125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 91.9897 | val_0_rmspe: 10.46094036102295|  0:00:11s\n",
      "epoch 10 | loss: 0.29749 | val_0_rmspe: 0.2660500109195709|  0:01:59s\n",
      "epoch 20 | loss: 0.26822 | val_0_rmspe: 0.2172500044107437|  0:03:48s\n",
      "epoch 30 | loss: 0.22493 | val_0_rmspe: 0.2010200023651123|  0:05:38s\n",
      "epoch 40 | loss: 0.22598 | val_0_rmspe: 0.20356999337673187|  0:07:30s\n",
      "epoch 50 | loss: 0.22108 | val_0_rmspe: 0.1890300065279007|  0:09:18s\n",
      "epoch 60 | loss: 0.22214 | val_0_rmspe: 0.20409999787807465|  0:11:03s\n",
      "epoch 70 | loss: 0.20823 | val_0_rmspe: 0.19384999573230743|  0:12:50s\n",
      "epoch 80 | loss: 0.20029 | val_0_rmspe: 0.18671000003814697|  0:14:42s\n",
      "epoch 90 | loss: 0.20183 | val_0_rmspe: 0.18686999380588531|  0:16:33s\n",
      "epoch 100| loss: 0.19829 | val_0_rmspe: 0.18700000643730164|  0:18:19s\n",
      "epoch 110| loss: 0.19437 | val_0_rmspe: 0.18351000547409058|  0:20:04s\n",
      "epoch 120| loss: 0.19467 | val_0_rmspe: 0.1840600073337555|  0:21:54s\n",
      "epoch 130| loss: 0.19375 | val_0_rmspe: 0.1840900033712387|  0:23:46s\n",
      "epoch 140| loss: 0.19239 | val_0_rmspe: 0.18351000547409058|  0:25:36s\n",
      "epoch 150| loss: 0.1925  | val_0_rmspe: 0.1859000027179718|  0:27:22s\n",
      "epoch 160| loss: 0.1916  | val_0_rmspe: 0.18310999870300293|  0:29:09s\n",
      "epoch 170| loss: 0.19122 | val_0_rmspe: 0.18320000171661377|  0:30:59s\n",
      "epoch 180| loss: 0.19105 | val_0_rmspe: 0.18296000361442566|  0:32:50s\n",
      "epoch 190| loss: 0.19103 | val_0_rmspe: 0.18344999849796295|  0:34:35s\n",
      "epoch 200| loss: 0.20296 | val_0_rmspe: 0.2048099935054779|  0:36:20s\n",
      "epoch 210| loss: 0.49667 | val_0_rmspe: 0.24085000157356262|  0:38:08s\n",
      "\n",
      "Early stopping occurred at epoch 215 with best_epoch = 165 and best_val_0_rmspe = 0.1823199987411499\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at artifacts/fold_0.pth.zip\n",
      "total 1 models will be used.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "del df, df_train\n",
    "gc.collect()\n",
    "\n",
    "if PREDICT_MLP:\n",
    "    model_paths = []\n",
    "    \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 30\n",
    "        valid_th = NN_VALID_TH\n",
    "    \n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        # MLP\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                                 [folds[-1]], \n",
    "                                                 device=device, \n",
    "                                                 batch_size=512,\n",
    "                                                 mlp_bn=True,\n",
    "                                                 mlp_hidden=256,\n",
    "                                                 mlp_dropout=0.0,\n",
    "                                                 emb_dim=30,\n",
    "                                                 epochs=epochs,\n",
    "                                                 lr=0.002,\n",
    "                                                 max_lr=0.0055,\n",
    "                                                 weight_decay=1e-7,\n",
    "                                                model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                                seed=i)\n",
    "\n",
    "        print(nn_losses)\n",
    "        if nn_losses[0] < NN_VALID_TH:\n",
    "            print(f'model of seed {i} added.')\n",
    "            model_paths.append(f'artifacts/mlp_fold_0_seed{i}.pth')\n",
    "            np.save(f'fold3_pred_mlp_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    mlp_model = [torch.load(path, device) for path in model_paths]\n",
    "    \n",
    "    print(f'total {len(mlp_model)} models will be used.')\n",
    "if PREDICT_CNN:\n",
    "    model_paths = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 50\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               cnn_hidden=8*128,\n",
    "                                                  batch_size=1280,\n",
    "                                                  model_type='cnn',\n",
    "                                                  emb_dim=30,\n",
    "                                                  epochs=epochs, #epochs,\n",
    "                                                  cnn_channel1=128,\n",
    "                                                  cnn_channel2=3*128,\n",
    "                                                  cnn_channel3=3*128,\n",
    "                                                  lr=0.00038, #0.0011,\n",
    "                                                  #scaler_type=scaler_type,\n",
    "                                                  max_lr=0.0013,\n",
    "                                                  weight_decay=6.5e-6,\n",
    "                                                optimizer_type='adam',\n",
    "                                               scheduler_type='reduce',\n",
    "                                                model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                                seed=i,\n",
    "                                              cnn_dropout=0.0,\n",
    "                                              cnn_weight_norm=True,\n",
    "                                              cnn_leaky_relu=False,\n",
    "                                              patience=8,\n",
    "                                              factor=0.3)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            model_paths.append(f'artifacts/cnn_fold_0_seed{i}.pth')\n",
    "            np.save(f'fold3_pred_cnn_seed{i}.npy', nn_preds[0])\n",
    "    cnn_model = [torch.load(path, device) for path in model_paths]\n",
    "        \n",
    "    print(f'total {len(cnn_model)} models will be used.')\n",
    "    \n",
    "if PREDICT_TABNET:\n",
    "    tab_model = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        epochs = 10\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 250\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(TABNET_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "                                                          [folds[-1]], \n",
    "                                                          batch_size=512,\n",
    "                                                          epochs=250, #epochs,\n",
    "                                                          lr=0.02,\n",
    "                                                          patience=50,\n",
    "                                                          factor=0.5,\n",
    "                                                          gamma=1.5,\n",
    "                                                          lambda_sparse=3e-7,\n",
    "                                                          seed=i)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            tab_model.append(model)\n",
    "            np.save(f'fold3_pred_tab_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    print(f'total {len(tab_model)} models will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39964ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:49:37.999935Z",
     "iopub.status.busy": "2021-09-23T13:49:37.999152Z",
     "iopub.status.idle": "2021-09-23T13:49:38.002524Z",
     "shell.execute_reply": "2021-09-23T13:49:38.002940Z"
    },
    "papermill": {
     "duration": 0.179039,
     "end_time": "2021-09-23T13:49:38.003097",
     "exception": false,
     "start_time": "2021-09-23T13:49:37.824058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f0a95",
   "metadata": {
    "papermill": {
     "duration": 0.035168,
     "end_time": "2021-09-23T13:49:38.070405",
     "exception": false,
     "start_time": "2021-09-23T13:49:38.035237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1c97718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:49:38.140886Z",
     "iopub.status.busy": "2021-09-23T13:49:38.140064Z",
     "iopub.status.idle": "2021-09-23T13:49:38.145288Z",
     "shell.execute_reply": "2021-09-23T13:49:38.145808Z"
    },
    "papermill": {
     "duration": 0.042851,
     "end_time": "2021-09-23T13:49:38.145938",
     "exception": false,
     "start_time": "2021-09-23T13:49:38.103087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 584)\n"
     ]
    }
   ],
   "source": [
    "X_test = get_X(df_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e949a565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:49:38.222509Z",
     "iopub.status.busy": "2021-09-23T13:49:38.221682Z",
     "iopub.status.idle": "2021-09-23T13:49:38.304572Z",
     "shell.execute_reply": "2021-09-23T13:49:38.305006Z"
    },
    "papermill": {
     "duration": 0.126679,
     "end_time": "2021-09-23T13:49:38.305133",
     "exception": false,
     "start_time": "2021-09-23T13:49:38.178454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to predict tab: Traceback (most recent call last):\n",
      "  File \"<ipython-input-21-d102e5fc0760>\", line 36, in <module>\n",
      "    tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD)\n",
      "  File \"<ipython-input-16-955a0c0c7349>\", line 542, in predict_tabnet\n",
      "    predicted.append(m.predict(X_processed))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py\", line 268, in predict\n",
      "    output, M_loss = self.network(data)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_network.py\", line 582, in forward\n",
      "    x = self.embedder(x)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pytorch_tabnet/tab_network.py\", line 853, in forward\n",
      "    post_embeddings = torch.cat(cols, dim=1)\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "\n",
      "prediction will be made by: []\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "df_pred = pd.DataFrame()\n",
    "df_pred['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "prediction_weights = {}\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "    gbdt_preds = booster.predict(X_test)\n",
    "    predictions['gbdt'] = gbdt_preds\n",
    "    prediction_weights['gbdt'] = 1\n",
    "\n",
    "\n",
    "if PREDICT_MLP and mlp_model:\n",
    "    try:\n",
    "        mlp_preds = predict_nn(X_test, mlp_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        predictions['mlp'] = mlp_preds\n",
    "        prediction_weights['mlp'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict mlp: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_CNN and cnn_model:\n",
    "    try:\n",
    "        cnn_preds = predict_nn(X_test, cnn_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        predictions['cnn'] = cnn_preds\n",
    "        prediction_weights['cnn'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict cnn: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_TABNET and tab_model:\n",
    "    try:\n",
    "        tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD)\n",
    "        predictions['tab'] = tab_preds\n",
    "        prediction_weights['tab'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict tab: {traceback.format_exc()}')\n",
    "\n",
    "        \n",
    "overall_preds = None\n",
    "overall_weight = np.sum(list(prediction_weights.values()))\n",
    "\n",
    "print(f'prediction will be made by: {list(prediction_weights.keys())}')\n",
    "\n",
    "for name, preds in predictions.items():\n",
    "    w = prediction_weights[name] / overall_weight\n",
    "    if overall_preds is None:\n",
    "        overall_preds = preds * w\n",
    "    else:\n",
    "        overall_preds += preds * w\n",
    "        \n",
    "df_pred['target'] = overall_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a7a26b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T13:49:38.374428Z",
     "iopub.status.busy": "2021-09-23T13:49:38.373945Z",
     "iopub.status.idle": "2021-09-23T13:49:38.394799Z",
     "shell.execute_reply": "2021-09-23T13:49:38.394344Z"
    },
    "papermill": {
     "duration": 0.057353,
     "end_time": "2021-09-23T13:49:38.394916",
     "exception": false,
     "start_time": "2021-09-23T13:49:38.337563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'sample_submission.csv'))\n",
    "submission = pd.merge(sub[['row_id']], df_pred[['row_id', 'target']], how='left')\n",
    "submission['target'] = submission['target'].fillna(0)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2926.51282,
   "end_time": "2021-09-23T13:49:40.790269",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-23T13:00:54.277449",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
